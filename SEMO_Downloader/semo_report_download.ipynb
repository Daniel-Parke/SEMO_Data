{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "from asyncio import Semaphore\n",
    "import os\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)  # Suppress httpx detailed logs\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReportURLs:\n",
    "    report_list_path: str = \"report_list_short.csv\"\n",
    "    reports: pd.DataFrame = field(init=False)\n",
    "    urls: Dict[str, List[str]] = field(default_factory=dict, init=False)\n",
    "    data: Dict[str, pd.DataFrame] = field(default_factory=dict, init=False)\n",
    "    semaphore: Semaphore = field(default=Semaphore(50), init=False)\n",
    "\n",
    "    async def async_init(self):\n",
    "        logging.info(\"Initializing ReportURLs class.\")\n",
    "        self.reports = pd.read_csv(self.report_list_path, dtype={'report_id': str})\n",
    "        await self.generate_all_urls_async()\n",
    "        await self.fetch_and_merge_data()\n",
    "\n",
    "    def create_api_url(self, report_num, page=\"1\", page_size=\"5000\"):\n",
    "        start_time = \"2010-11-18T00:00\"\n",
    "        end_time = \"2030-02-18T23:59\"\n",
    "        sort_by = \"StartTime\"\n",
    "        order_by = \"ASC\"\n",
    "        participant_name = \"\"\n",
    "        resource_name = \"\"\n",
    "        resource_type = \"\"\n",
    "        base_url = f\"https://reports.sem-o.com/api/v1/dynamic/BM-{report_num}?\"\n",
    "\n",
    "        url = (\n",
    "            f\"{base_url}\"\n",
    "            f\"StartTime=%3E%3D{start_time}&\"\n",
    "            f\"EndTime=%3C%3D{end_time}&\"\n",
    "            f\"sort_by={sort_by}&\"\n",
    "            f\"order_by={order_by}&\"\n",
    "            f\"ParticipantName={participant_name}&\"\n",
    "            f\"ResourceName={resource_name}&\"\n",
    "            f\"ResourceType={resource_type}&\"\n",
    "            f\"page={page}&\"\n",
    "            f\"page_size={page_size}\"\n",
    "        )\n",
    "\n",
    "        return url\n",
    "\n",
    "    async def get_total_pages(self, url):\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(url)\n",
    "            response_data = response.json()\n",
    "            total_pages = response_data['pagination']['totalPages']\n",
    "            return total_pages\n",
    "\n",
    "    async def generate_all_urls_async(self):\n",
    "        logging.info(\"Asynchronously generating all URLs for reports.\")\n",
    "        self.urls = {}\n",
    "        tasks = []\n",
    "        for index, row in self.reports.iterrows():\n",
    "            report_id = row['report_id']\n",
    "            report_name = row['name']\n",
    "            task = asyncio.create_task(self.fetch_total_pages_and_generate_urls(report_id, report_name))\n",
    "            tasks.append(task)\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    async def fetch_total_pages_and_generate_urls(self, report_id, report_name):\n",
    "        initial_url = self.create_api_url(report_id)\n",
    "        total_pages = await self.get_total_pages(initial_url)\n",
    "        report_urls = [self.create_api_url(report_id, page=str(page)) for page in range(1, total_pages + 1)]\n",
    "        self.urls[report_name] = report_urls\n",
    "    \n",
    "    async def fetch_with_retry(self, url, url_index, total_urls, report_name):\n",
    "        retries = 3\n",
    "        backoff_factor = 0.5\n",
    "        async with self.semaphore:\n",
    "            for attempt in range(1, retries + 1):\n",
    "                try:\n",
    "                    async with httpx.AsyncClient(timeout=20.0) as client:\n",
    "                        response = await client.get(url)\n",
    "                        response.raise_for_status()\n",
    "                        logging.info(f\"Request {url_index}/{total_urls} completed for {report_name}.\")\n",
    "                        return response.json()\n",
    "                except (httpx.ReadTimeout, httpx.ConnectTimeout) as e:\n",
    "                    logging.warning(f\"Timeout on attempt {attempt} for {url}: {e}\")\n",
    "                    if attempt == retries:\n",
    "                        logging.error(f\"Max retries reached for {url}.\")\n",
    "                        return None\n",
    "                    sleep_duration = backoff_factor * (2 ** (attempt - 1))\n",
    "                    logging.info(f\"Retrying in {sleep_duration} seconds...\")\n",
    "                    await asyncio.sleep(sleep_duration)\n",
    "                except httpx.RequestError as e:\n",
    "                    logging.error(f\"Request error for {url}: {e}\")\n",
    "                    return None\n",
    "\n",
    "    async def fetch_and_merge_data(self):\n",
    "        logging.info(\"Starting to fetch and merge data for all reports.\")\n",
    "        for report_name, urls in self.urls.items():\n",
    "            logging.info(f\"Processing {len(urls)} URLs for {report_name}.\")\n",
    "            tasks = [self.fetch_with_retry(url, i+1, len(urls), report_name) for i, url in enumerate(urls)]\n",
    "            pages_data = await asyncio.gather(*tasks)\n",
    "            combined_data = self.combine_data(pages_data)\n",
    "            if combined_data:\n",
    "                df = pd.DataFrame(combined_data)\n",
    "                self.save_data_to_csv(df, report_name)  # Call save method immediately after data is ready\n",
    "                logging.info(f\"Completed download and merging for {report_name}: {len(combined_data)} items fetched.\")\n",
    "            else:\n",
    "                logging.warning(f\"No data found for {report_name}.\")\n",
    "\n",
    "    def combine_data(self, pages_data):\n",
    "        all_data = []\n",
    "        for page_data in pages_data:\n",
    "            if page_data and 'items' in page_data:\n",
    "                all_data.extend(page_data['items'])\n",
    "        return all_data\n",
    "\n",
    "    def save_data_to_csv(self, df, report_name):\n",
    "        output_dir = \"Output_Data\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        file_path = os.path.join(output_dir, f\"{report_name}.csv\")\n",
    "        df.to_csv(file_path, index=False)\n",
    "        logging.info(f\"Saved {report_name} data to {file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 23:51:57,042 - INFO - Initializing ReportURLs class.\n",
      "2024-02-18 23:51:57,044 - INFO - Asynchronously generating all URLs for reports.\n",
      "2024-02-18 23:52:01,457 - INFO - Starting to fetch and merge data for all reports.\n",
      "2024-02-18 23:52:01,458 - INFO - Processing 1 URLs for Trading_Day_Ex_Rate.\n",
      "2024-02-18 23:52:02,820 - INFO - Request 1/1 completed for Trading_Day_Ex_Rate.\n",
      "2024-02-18 23:52:02,824 - INFO - Saved Trading_Day_Ex_Rate data to Output_Data\\Trading_Day_Ex_Rate.csv.\n",
      "2024-02-18 23:52:02,825 - INFO - Completed download and merging for Trading_Day_Ex_Rate: 188 items fetched.\n",
      "2024-02-18 23:52:02,825 - INFO - Processing 1 URLs for Daily_Load_Forecast_Summary.\n",
      "2024-02-18 23:52:03,242 - INFO - Request 1/1 completed for Daily_Load_Forecast_Summary.\n",
      "2024-02-18 23:52:03,265 - INFO - Saved Daily_Load_Forecast_Summary data to Output_Data\\Daily_Load_Forecast_Summary.csv.\n",
      "2024-02-18 23:52:03,266 - INFO - Completed download and merging for Daily_Load_Forecast_Summary: 4512 items fetched.\n",
      "2024-02-18 23:52:03,266 - INFO - Processing 1 URLs for Demand_Control.\n",
      "2024-02-18 23:52:04,237 - INFO - Request 1/1 completed for Demand_Control.\n",
      "2024-02-18 23:52:04,253 - INFO - Saved Demand_Control data to Output_Data\\Demand_Control.csv.\n",
      "2024-02-18 23:52:04,254 - INFO - Completed download and merging for Demand_Control: 4320 items fetched.\n",
      "2024-02-18 23:52:04,254 - INFO - Processing 1 URLs for Imbalance_Price_Report(Imbalance_Settlement_Period).\n",
      "2024-02-18 23:52:05,041 - INFO - Request 1/1 completed for Imbalance_Price_Report(Imbalance_Settlement_Period).\n",
      "2024-02-18 23:52:05,067 - INFO - Saved Imbalance_Price_Report(Imbalance_Settlement_Period) data to Output_Data\\Imbalance_Price_Report(Imbalance_Settlement_Period).csv.\n",
      "2024-02-18 23:52:05,068 - INFO - Completed download and merging for Imbalance_Price_Report(Imbalance_Settlement_Period): 4416 items fetched.\n",
      "2024-02-18 23:52:05,068 - INFO - Processing 2 URLs for Aggregated_Wind_Forecast.\n",
      "2024-02-18 23:52:05,746 - INFO - Request 1/2 completed for Aggregated_Wind_Forecast.\n",
      "2024-02-18 23:52:05,755 - INFO - Request 2/2 completed for Aggregated_Wind_Forecast.\n",
      "2024-02-18 23:52:05,792 - INFO - Saved Aggregated_Wind_Forecast data to Output_Data\\Aggregated_Wind_Forecast.csv.\n",
      "2024-02-18 23:52:05,793 - INFO - Completed download and merging for Aggregated_Wind_Forecast: 9200 items fetched.\n",
      "2024-02-18 23:52:05,794 - INFO - Processing 1 URLs for Forecast_Imbalance.\n",
      "2024-02-18 23:52:06,284 - INFO - Request 1/1 completed for Forecast_Imbalance.\n",
      "2024-02-18 23:52:06,321 - INFO - Saved Forecast_Imbalance data to Output_Data\\Forecast_Imbalance.csv.\n",
      "2024-02-18 23:52:06,322 - INFO - Completed download and merging for Forecast_Imbalance: 4418 items fetched.\n",
      "2024-02-18 23:52:06,322 - INFO - Processing 12 URLs for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,437 - INFO - Request 12/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,465 - INFO - Request 4/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,601 - INFO - Request 8/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,624 - INFO - Request 10/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,645 - INFO - Request 11/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,668 - INFO - Request 5/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,698 - INFO - Request 6/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,709 - INFO - Request 1/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,722 - INFO - Request 9/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,730 - INFO - Request 7/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,738 - INFO - Request 2/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,775 - INFO - Request 3/12 completed for Aggregated_Contracted_Wind_Quantities.\n",
      "2024-02-18 23:52:09,966 - INFO - Saved Aggregated_Contracted_Wind_Quantities data to Output_Data\\Aggregated_Contracted_Wind_Quantities.csv.\n",
      "2024-02-18 23:52:09,967 - INFO - Completed download and merging for Aggregated_Contracted_Wind_Quantities: 55263 items fetched.\n",
      "2024-02-18 23:52:09,967 - INFO - Processing 6 URLs for Imbalance_Price_Report(Imbalance_Pricing_Period).\n",
      "2024-02-18 23:52:11,579 - INFO - Request 6/6 completed for Imbalance_Price_Report(Imbalance_Pricing_Period).\n",
      "2024-02-18 23:52:12,000 - INFO - Request 2/6 completed for Imbalance_Price_Report(Imbalance_Pricing_Period).\n",
      "2024-02-18 23:52:12,028 - INFO - Request 3/6 completed for Imbalance_Price_Report(Imbalance_Pricing_Period).\n",
      "2024-02-18 23:52:12,168 - INFO - Request 1/6 completed for Imbalance_Price_Report(Imbalance_Pricing_Period).\n",
      "2024-02-18 23:52:12,203 - INFO - Request 5/6 completed for Imbalance_Price_Report(Imbalance_Pricing_Period).\n",
      "2024-02-18 23:52:12,413 - INFO - Request 4/6 completed for Imbalance_Price_Report(Imbalance_Pricing_Period).\n",
      "2024-02-18 23:52:12,688 - INFO - Saved Imbalance_Price_Report(Imbalance_Pricing_Period) data to Output_Data\\Imbalance_Price_Report(Imbalance_Pricing_Period).csv.\n",
      "2024-02-18 23:52:12,688 - INFO - Completed download and merging for Imbalance_Price_Report(Imbalance_Pricing_Period): 26499 items fetched.\n",
      "2024-02-18 23:52:12,689 - INFO - Processing 1 URLs for Unit_Under_Test.\n",
      "2024-02-18 23:52:13,000 - INFO - Request 1/1 completed for Unit_Under_Test.\n",
      "2024-02-18 23:52:13,013 - INFO - Saved Unit_Under_Test data to Output_Data\\Unit_Under_Test.csv.\n",
      "2024-02-18 23:52:13,014 - INFO - Completed download and merging for Unit_Under_Test: 53 items fetched.\n",
      "2024-02-18 23:52:13,014 - INFO - Processing 1 URLs for Balance_&_Imbalance_Market_Cost.\n",
      "2024-02-18 23:52:13,432 - INFO - Request 1/1 completed for Balance_&_Imbalance_Market_Cost.\n",
      "2024-02-18 23:52:13,458 - INFO - Saved Balance_&_Imbalance_Market_Cost data to Output_Data\\Balance_&_Imbalance_Market_Cost.csv.\n",
      "2024-02-18 23:52:13,459 - INFO - Completed download and merging for Balance_&_Imbalance_Market_Cost: 4415 items fetched.\n",
      "2024-02-18 23:52:13,459 - INFO - Processing 1 URLs for Interconnector_Flows_&_Residual_Capacity.\n",
      "2024-02-18 23:52:13,844 - INFO - Request 1/1 completed for Interconnector_Flows_&_Residual_Capacity.\n",
      "2024-02-18 23:52:13,862 - INFO - Saved Interconnector_Flows_&_Residual_Capacity data to Output_Data\\Interconnector_Flows_&_Residual_Capacity.csv.\n",
      "2024-02-18 23:52:13,863 - INFO - Completed download and merging for Interconnector_Flows_&_Residual_Capacity: 2704 items fetched.\n",
      "2024-02-18 23:52:13,863 - INFO - Processing 12 URLs for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:16,834 - INFO - Request 12/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:16,893 - INFO - Request 4/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,034 - INFO - Request 5/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,055 - INFO - Request 1/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,063 - INFO - Request 7/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,095 - INFO - Request 11/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,195 - INFO - Request 8/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,200 - INFO - Request 10/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,261 - INFO - Request 3/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,271 - INFO - Request 9/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,276 - INFO - Request 6/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,307 - INFO - Request 2/12 completed for Aggregated_Contracted_Demand_Quantities.\n",
      "2024-02-18 23:52:17,496 - INFO - Saved Aggregated_Contracted_Demand_Quantities data to Output_Data\\Aggregated_Contracted_Demand_Quantities.csv.\n",
      "2024-02-18 23:52:17,497 - INFO - Completed download and merging for Aggregated_Contracted_Demand_Quantities: 55225 items fetched.\n",
      "2024-02-18 23:52:17,497 - INFO - Processing 12 URLs for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,445 - INFO - Request 12/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,643 - INFO - Request 11/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,793 - INFO - Request 5/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,805 - INFO - Request 9/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,810 - INFO - Request 8/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,844 - INFO - Request 2/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,853 - INFO - Request 3/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,883 - INFO - Request 7/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,897 - INFO - Request 10/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,904 - INFO - Request 1/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:20,911 - INFO - Request 4/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:21,161 - INFO - Request 6/12 completed for Aggregated_Contracted_Generation_Quantities.\n",
      "2024-02-18 23:52:21,354 - INFO - Saved Aggregated_Contracted_Generation_Quantities data to Output_Data\\Aggregated_Contracted_Generation_Quantities.csv.\n",
      "2024-02-18 23:52:21,355 - INFO - Completed download and merging for Aggregated_Contracted_Generation_Quantities: 55239 items fetched.\n",
      "2024-02-18 23:52:21,355 - INFO - Processing 1 URLs for Average_System_Frequency.\n",
      "2024-02-18 23:52:21,769 - INFO - Request 1/1 completed for Average_System_Frequency.\n",
      "2024-02-18 23:52:21,803 - INFO - Saved Average_System_Frequency data to Output_Data\\Average_System_Frequency.csv.\n",
      "2024-02-18 23:52:21,804 - INFO - Completed download and merging for Average_System_Frequency: 4224 items fetched.\n",
      "2024-02-18 23:52:21,804 - INFO - Processing 2 URLs for Four_Day_Aggregated_Wind_Forecast.\n",
      "2024-02-18 23:52:22,527 - INFO - Request 1/2 completed for Four_Day_Aggregated_Wind_Forecast.\n",
      "2024-02-18 23:52:22,548 - INFO - Request 2/2 completed for Four_Day_Aggregated_Wind_Forecast.\n",
      "2024-02-18 23:52:22,600 - INFO - Saved Four_Day_Aggregated_Wind_Forecast data to Output_Data\\Four_Day_Aggregated_Wind_Forecast.csv.\n",
      "2024-02-18 23:52:22,600 - INFO - Completed download and merging for Four_Day_Aggregated_Wind_Forecast: 9200 items fetched.\n"
     ]
    }
   ],
   "source": [
    "report_urls = ReportURLs()\n",
    "await report_urls.async_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 URL's generated for Unit_Under_Test\n",
      "1 URL's generated for Trading_Day_Ex_Rate\n",
      "281 URL's generated for Dispatch_Quantity\n",
      "12 URL's generated for Aggregated_Contracted_Generation_Quantities\n",
      "1 URL's generated for Imbalance_Price_Report(Imbalance_Settlement_Period)\n",
      "2 URL's generated for Aggregated_Wind_Forecast\n",
      "12 URL's generated for Aggregated_Contracted_Demand_Quantities\n",
      "66 URL's generated for Daily_Dispatch_Instructions_D+1\n",
      "1 URL's generated for Interconnector_Flows_&_Residual_Capacity\n",
      "12 URL's generated for Aggregated_Contracted_Wind_Quantities\n",
      "1 URL's generated for Demand_Control\n",
      "1 URL's generated for Forecast_Imbalance\n",
      "1 URL's generated for Balance_&_Imbalance_Market_Cost\n",
      "1 URL's generated for Average_System_Frequency\n",
      "2 URL's generated for Four_Day_Aggregated_Wind_Forecast\n",
      "1 URL's generated for Daily_Load_Forecast_Summary\n",
      "6 URL's generated for Imbalance_Price_Report(Imbalance_Pricing_Period)\n",
      "89 URL's generated for Final_Physical_Notificaiton\n",
      "137 URL's generated for Forecast_Availability\n",
      "63 URL's generated for Daily_Dispatch_Instructions_D+4\n",
      "285 URL's generated for Anonymised_IncDec_Curve\n",
      "283 URL's generated for Average_Outturn_Availability\n",
      "197 URL's generated for Generator_Unit_Technical_Characteristics_Transactions\n",
      "735 URL's generated for Daily_Meter_Data\n"
     ]
    }
   ],
   "source": [
    "for key in report_urls.urls.keys():\n",
    "    print(f\"{len(report_urls.urls[key])} URL's generated for {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    current_directory = os.path.dirname(os.path.abspath(\"web_scrape.ipynb\"))\n",
    "    setup_and_download_reports(current_directory)\n",
    "    merge_data(current_directory)\n",
    "\n",
    "def setup_and_download_reports(current_directory):\n",
    "    \"\"\"Sets up directories and downloads reports based on a CSV file.\"\"\"\n",
    "    downloads_main_dir = create_main_download_directory(current_directory)\n",
    "    download_dir = create_versioned_download_directory(downloads_main_dir)\n",
    "    csv_file = os.path.join(current_directory, 'report_list.csv')\n",
    "    download_reports(download_dir, csv_file)\n",
    "\n",
    "def merge_data(current_directory):\n",
    "    \"\"\"Merges CSV files from download directories and saves them into an output directory.\"\"\"\n",
    "    base_dir = os.path.join(current_directory, 'Downloads')\n",
    "    output_dir = os.path.join(current_directory, 'Output_Data')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    data_folders = [folder for folder in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, folder)) and 'Downloaded_Data_' in folder]\n",
    "    data_folders += [output_dir]  # Include Output_Data for merging\n",
    "\n",
    "    merged_dataframes = {}\n",
    "\n",
    "    for folder_name in data_folders:\n",
    "        folder_path = os.path.join(base_dir, folder_name) if folder_name != os.path.basename(output_dir) else output_dir\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            new_df = pd.read_csv(file_path)\n",
    "\n",
    "            if file_name in merged_dataframes:\n",
    "                merged_dataframes[file_name] = pd.concat([merged_dataframes[file_name], new_df], ignore_index=True)\n",
    "            else:\n",
    "                merged_dataframes[file_name] = new_df\n",
    "\n",
    "    for file_name, df in merged_dataframes.items():\n",
    "        df = df.drop_duplicates()\n",
    "        sort_column = 'StartTime' if 'StartTime' in df.columns else 'TradeDate' if 'TradeDate' in df.columns else None\n",
    "        if sort_column:\n",
    "            df = df.sort_values(by=sort_column)\n",
    "\n",
    "        df.to_csv(os.path.join(output_dir, file_name), index=False)\n",
    "\n",
    "    print(f'Merged files are saved in {output_dir}.')\n",
    "\n",
    "def create_main_download_directory(current_directory):\n",
    "    \"\"\"Creates a main download directory.\"\"\"\n",
    "    downloads_main_dir = os.path.join(current_directory, \"Downloads\")\n",
    "    if not os.path.exists(downloads_main_dir):\n",
    "        os.makedirs(downloads_main_dir)\n",
    "    return downloads_main_dir\n",
    "\n",
    "def create_versioned_download_directory(downloads_main_dir):\n",
    "    \"\"\"Creates a versioned download directory within the main download directory with date and version.\"\"\"\n",
    "    today = datetime.now()\n",
    "    date_str = today.strftime(\"%d_%m_%Y\")\n",
    "    dir_number = 1\n",
    "    while True:\n",
    "        dir_name = f\"Downloaded_Data_{date_str}_{dir_number}\"\n",
    "        download_dir = os.path.join(downloads_main_dir, dir_name)\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "            return download_dir\n",
    "        dir_number += 1\n",
    "\n",
    "def download_reports(download_dir, csv_file):\n",
    "    \"\"\"Downloads reports based on a given CSV file.\"\"\"\n",
    "    dataframe = pd.read_csv(csv_file, dtype={'report_id': str})\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_experimental_option(\"prefs\", {\n",
    "        \"download.default_directory\": download_dir,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"safebrowsing.enabled\": False,\n",
    "    })\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    reports_downloaded = 0\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        report_id = row['report_id']\n",
    "        report_name = row['name']\n",
    "        chart_required = row['chart?'].lower() == 'y'\n",
    "        url = f\"https://www.sem-o.com/market-data/dynamic-reports/#BM-{report_id}\"\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            if chart_required:\n",
    "                wait_and_click(driver, By.ID, \"dynamic-reports-table\")\n",
    "            wait_and_click(driver, By.CSS_SELECTOR, \".icon.icon-download\")\n",
    "            print(f\"Downloaded report ID: {report_id} - {report_name}.\")\n",
    "            reports_downloaded += 1\n",
    "        except (TimeoutException, ElementClickInterceptedException) as e:\n",
    "            print(f\"Failed to interact with the elements for report ID {report_id} - {report_name}. Reason: {e}\")\n",
    "            continue\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    print(f\"Successfully downloaded {reports_downloaded}/{len(dataframe)} reports.\")\n",
    "    driver.quit()\n",
    "\n",
    "def wait_and_click(driver, by_method, selector):\n",
    "    \"\"\"Waits for an element to be clickable and then clicks it.\"\"\"\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((by_method, selector)))\n",
    "        element.click()\n",
    "    except ElementClickInterceptedException as e:\n",
    "        driver.execute_script(\"arguments[0].click();\", element)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
